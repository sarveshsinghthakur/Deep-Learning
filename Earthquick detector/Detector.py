# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19RvwTX1kjXlKW6dpOuC1dcjxngChv0SH
"""

!pip install basemap pyproj matplotlib

"""**Import Libraries**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

"""**Load Data**"""

data = pd.read_csv("https://raw.githubusercontent.com/amankharwal/Website-data/master/database.csv")
data = data[['Date', 'Time', 'Latitude', 'Longitude', 'Depth', 'Magnitude']]

import datetime
import time

timestamp = []
for d, t in zip(data['Date'], data['Time']):
    try:
        ts = datetime.datetime.strptime(d + ' ' + t, '%m/%d/%Y %H:%M:%S')
        timestamp.append(time.mktime(ts.timetuple()))
    except ValueError:
        timestamp.append('ValueError')

data['Timestamp'] = pd.Series(timestamp)
final_data = data.drop(['Date', 'Time'], axis=1)
final_data = final_data[final_data.Timestamp != 'ValueError'].copy()
final_data['Timestamp'] = pd.to_numeric(final_data['Timestamp'])

"""**Visualize Data**"""

m = Basemap(projection='mill', llcrnrlat=-80, urcrnrlat=80,
            llcrnrlon=-180, urcrnrlon=180, lat_ts=20, resolution='c')

longitudes = data["Longitude"].tolist()
latitudes = data["Latitude"].tolist()

x, y = m(longitudes, latitudes)

fig = plt.figure(figsize=(12, 10))
plt.title("All Affected Areas")
m.plot(x, y, "o", markersize=2, color='red')
m.drawcoastlines(color='black')
m.fillcontinents(color='green', lake_color='blue')
m.drawmapboundary(color='k')
m.drawcountries(color='black')
plt.show()

"""**Prepare Data**"""

x = final_data[['Timestamp', 'Latitude', 'Longitude']]
y = final_data[['Depth', 'Magnitude']]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

"""**Define Model**"""

model = Sequential([
    Input(shape=(x_train.shape[1],)),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dense(y_train.shape[1], activation='linear')
])

"""**Compile and Train**"""

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])

callbacks = [
    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
]

history = model.fit(
    x_train, y_train,
    validation_split=0.2,
    epochs=150,
    batch_size=32,
    callbacks=callbacks,
    verbose=1
)

"""**Evaluate Model**"""

loss, mae = model.evaluate(x_test, y_test, verbose=0)
print(f"Test MSE: {loss:.4f} | Test MAE: {mae:.4f}")

"""**Make Predictions**"""

y_pred = model.predict(x_test)
print("Sample predictions:")
print(pd.DataFrame(y_pred[:5], columns=['Predicted Depth', 'Predicted Magnitude']))

"""**Plot Training History**"""

plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('MSE')
plt.legend()
plt.show()