# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16b-kZUGNCrNhE8SPDHCXkpoJxOkv9Ons
"""

!pip install tensorflow keras faker

from tensorflow import keras
import tensorflow as tf
import pandas as pd
import os
import re
from tensorflow.keras.preprocessing.text import Tokenizer, sequence
from keras.layers.embeddings import Embedding
from keras.utils import to_categorical
import numpy as np
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import Callback
from faker import Faker
from sklearn.model_selection import train_test_split

female_data = pd.read_csv('/content/Indian-Female-Names.csv')
male_data = pd.read_csv("/content/Indian-Male-Names.csv")

repl_list = ['s/o','d/o','w/o','/','&',',','-']

def clean_data(name):
        name = str(name).lower()
        name = (''.join(i for i in name if ord(i)<128)).strip()
        for repl in repl_list:
                name = name.replace(repl," ")
        if '@' in name:
                pos = name.find('@')
                name = name[:pos].strip()
        name = name.split(" ")
        name = " ".join([each.strip() for each in name])
        return name

def remove_records(merged_data):
        merged_data['delete'] = 0
        merged_data.loc[merged_data['name'].str.find('with') != -1,'delete'] = 1
        merged_data.loc[merged_data['count_words']>=5,'delete']=1
        merged_data.loc[merged_data['count_words']==0,'delete']=1
        merged_data.loc[merged_data['name'].str.contains(r'\d') == True,'delete']=1
        cleaned_data = merged_data[merged_data.delete==0]
        return cleaned_data

merged_data = pd.concat((male_data,female_data),axis=0)

merged_data['name'] = merged_data['name'].apply(clean_data)
merged_data['count_words'] = merged_data['name'].str.split().apply(len)

cleaned_data = remove_records(merged_data)

indian_cleaned_data = cleaned_data[['name','count_words']].drop_duplicates(subset='name',keep='first')
indian_cleaned_data['label'] = 'indian'

len(indian_cleaned_data)

fake = Faker("en_us")
non_indian_data_list = []
for i in range(14000):
  name = fake.name()
  non_indian_data_list.append({'name':name,'count_words':len(name.split()),'label':'non_indian'})

non_indian_data = pd.DataFrame(non_indian_data_list)
len(non_indian_data)

all_names = pd.concat([indian_cleaned_data, non_indian_data], ignore_index=True)
train_data, test_data = train_test_split(all_names, test_size=0.2, random_state=42)

print(f"Training data shape: {train_data.shape}")
print(f"Testing data shape: {test_data.shape}")

max_words = 10000
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(train_data['name'])
train_sequences = tokenizer.texts_to_sequences(train_data['name'])
test_sequences = tokenizer.texts_to_sequences(test_data['name'])
max_sequence_length = 10
train_padded = sequence.pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')
test_padded = sequence.pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')

print(f"Shape of training sequences: {train_padded.shape}")
print(f"Shape of testing sequences: {test_padded.shape}")

label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(train_data['label'])
test_labels_encoded = label_encoder.transform(test_data['label'])
train_labels_one_hot = to_categorical(train_labels_encoded)
test_labels_one_hot = to_categorical(test_labels_encoded)

print(f"Shape of training labels: {train_labels_one_hot.shape}")
print(f"Shape of testing labels: {test_labels_one_hot.shape}")

model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))
model.add(LSTM(128))
model.add(Dense(64, activation='relu'))
model.add(Dense(train_labels_one_hot.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

history = model.fit(train_padded, train_labels_one_hot, epochs=10, batch_size=32, validation_split=0.2)

loss, accuracy = model.evaluate(test_padded, test_labels_one_hot, verbose=0)

print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

def predict_name_origin(names):
    cleaned_names = [clean_data(name) for name in names]
    name_sequences = tokenizer.texts_to_sequences(cleaned_names)
    name_padded = sequence.pad_sequences(name_sequences, maxlen=max_sequence_length, padding='post', truncating='post')
    predictions = model.predict(name_padded)
    predicted_labels_encoded = np.argmax(predictions, axis=1)
    predicted_labels = label_encoder.inverse_transform(predicted_labels_encoded)
    return predicted_labels

while True:
  input_names_string = input('Enter a list of names separated by commas (or type "exit" to quit): ')
  if input_names_string.lower() == 'exit':
    break
  new_names = input_names_string.split(',')
  predicted_origins = predict_name_origin(new_names)

  for name, origin in zip(new_names, predicted_origins):
      print(f"The name '{name}' is predicted as: {origin}")